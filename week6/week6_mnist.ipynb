{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week6_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0nOZ-NcQIec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9030ce24-ddaf-4c40-b5b9-63100933a836"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "print(\"library importing done\")\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self . fc1 = nn.Linear(784,512)\n",
        "        self . fc2 = nn.Linear(512,256)\n",
        "        self . fc3 = nn.Linear(256,128)\n",
        "        self . fc4 = nn.Linear(128,64)\n",
        "        self . fc5 = nn.Linear(64,32)\n",
        "        self . fc6 = nn.Linear(32,10)\n",
        "    \n",
        "    \n",
        "    def forword(self, x):\n",
        "        x= x.float()\n",
        "        h1 = F.relu(self.fc1(x.view(-1,784)))\n",
        "        h2 = F.relu(self.fc2(h1))\n",
        "        h3 = F.relu(self.fc3(h2))\n",
        "        h4 = F.relu(self.fc4(h3))\n",
        "        h5 = F.relu(self.fc5(h4))\n",
        "        h6 = self.fc6(h5)\n",
        "        return  F.log_softmax(h6 , dim=1) \n",
        "    \n",
        "print(\"init model done\") "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "library importing done\n",
            "init model done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THI7yCtacJ65",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "7d244831-17b3-4bd6-e81c-dd4bab7ce2e4"
      },
      "source": [
        "batch_size = 64\n",
        "test_batch_size = 1000\n",
        "epochs = 10\n",
        "lr = 0.01\n",
        "momentum = 0.5\n",
        "co_cuda = True\n",
        "seed = 1\n",
        "log_interval = 200\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,) , (0.3081,))])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('/Users/yijinshin/OneDrive/2019_2/project/week6' , train=True , download=True , transform=transform) , \n",
        "    batch_size = batch_size , shuffle = True , **kwargs)\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters() , lr = lr , momentum=momentum)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5b033bafdf8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m train_loader = torch.utils.data.DataLoader(\n\u001b[1;32m     15\u001b[0m     \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/yijinshin/OneDrive/2019_2/project/week6'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     batch_size = batch_size , shuffle = True , **kwargs)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'kwargs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FWa3lNzeMPK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "667b8eec-8717-4578-fbec-e236a8176816"
      },
      "source": [
        "def train( log_interval , model , device , train_loader , optimizer , epoch) :\n",
        "    model.train() #모델을 트레이닝 하겠다는 의미 \n",
        "    \n",
        "    for batch_idx , (data , target) in enumerate (train_loader) :     #몇번째 batch인지 알려주고(몇번째 읽는건지) 우리는 batch를 64로 했으니까 data와 target을 64개씩 가져올거임\n",
        "        data , target = data.to(device) , target.to(device)      #가져온 data, target을 device에서 사용할 것이라고 명시\n",
        "        optimizer.zero_grad()     #옵티마이저가 loss를 backword를 하고 그 안에 있는 혹시 남아있는 값이 있다면 그걸 초기화 하기 위한 것 \n",
        "        output = model(data)    #모델에 데이터를 넣고, 그 아웃풋 받기\n",
        "        loss = F.nll_loss(output , target)    #나온 아웃풋을 target과 비교해서 나온 로스값을 받는다.\n",
        "        loss.backward()    #그 loss값을 통해 backpropagation 진행\n",
        "        optimizer.step()   # weight 업데이트\n",
        "        if batch_idx % log_interval ==0 :  #우리가 설정한  log_interval마다 진행 상황 프린트 (우리는 200으로 했음)\n",
        "            print('Train Epoch : {} [{}/{} ({:.0f}%)]\\tLoss : {:.6f}' . format( epoch , batch_idx * len(data) , len(train_loader.dataset) , 100.*batch_idx / len(train_loader) , loss.item()))\n",
        "            \n",
        "            \n",
        "            \n",
        "def test ( log_interval , model , device , test_loader) : \n",
        "    model.eval()   #test를 할떄도 모델을 evaluation한다.\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad() :\n",
        "        for data , target in test_loader :     #1000개씩 넣기로 함\n",
        "            data , target = data.to(device) , target.to(device)\n",
        "            output = model(data)    #모델안에 data넣기\n",
        "            test_loss += F.null_loss(output , target , reduction = 'sum').item()    #output이랑 target이랑 비교해서 loss 나온걸 계속 total_loss에 더함. 1000개씩 확인하고 그 결과를 더하고 그러는거임. 결국은 그 평균을 볼거라서 (5000개를 1000개씩 5번 넣고 결과를 다 더하고 5로 나눈 그 값 사용)\n",
        "            pred = output.argmax(dim = 1 , keepdim = True)    #output에서 argmax하면 모델이 예측한 숫자 값이 나옴\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()    #모델이 예측한 숫자와 타겟이 같으면 correct에 더하기 해주기. correct는 결국 얼마나 모델이 잘 맞췄는지에대한 정확도임\n",
        "            \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    \n",
        "    print('\\nTest set : Average loss : {:.4f} , Accuracy : {}/{} ({:.0f}%)\\n'.format(test_loss , correct , len(Test_loader.dataset) , 100. *correct / len(test_loader.dataset)))\n",
        "\n",
        "print(\"done\")\n",
        "        "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gqbGzgOkMHl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "fbd76143-dc8e-4de3-92f3-fcc18509428f"
      },
      "source": [
        "# data와 target이 어떤 모습으로 나오는지 보여주려고 한 batch만큼만 보여줌\n",
        "for batch , (data , target) in enumerate(train_loader) :\n",
        "    print(data , target)\n",
        "    break"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f365a8722eff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ4nEw3JkVQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#본격적으로 epoch만큼 training하고 test하기\n",
        "\n",
        "for epoch in range(1,11) : \n",
        "    train(log_interval , model , device , train_loader , optimizer , epoch)\n",
        "    test(log_interval , model , device , test_loader)\n",
        "    \n",
        "    #출력에서 loss 값이 감소해야지 training이 잘 되는거임. 이거 잘 보기\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}